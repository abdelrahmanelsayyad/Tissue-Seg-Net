{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:02:47.907091Z\",\"iopub.execute_input\":\"2025-05-10T21:02:47.907845Z\",\"iopub.status.idle\":\"2025-05-10T21:03:04.640671Z\",\"shell.execute_reply.started\":\"2025-05-10T21:02:47.907811Z\",\"shell.execute_reply\":\"2025-05-10T21:03:04.640048Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom tqdm import tqdm\nimport cv2\nimport itertools\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.metrics import confusion_matrix\n\n%matplotlib inline\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:03:04.644577Z\",\"iopub.execute_input\":\"2025-05-10T21:03:04.644834Z\",\"iopub.status.idle\":\"2025-05-10T21:03:04.964595Z\",\"shell.execute_reply.started\":\"2025-05-10T21:03:04.644809Z\",\"shell.execute_reply\":\"2025-05-10T21:03:04.963686Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport os\nfrom glob import glob\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"leoscode/wound-segmentation-images\")\n\n# Define paths based on downloaded directory\ntrain_image_dir = os.path.join(path, 'data_wound_seg', 'train_images')\ntrain_mask_dir = os.path.join(path, 'data_wound_seg', 'train_masks')\ntest_image_dir = os.path.join(path, 'data_wound_seg', 'test_images')\ntest_mask_dir = os.path.join(path, 'data_wound_seg', 'test_masks')\ncorrespondence_table_path = os.path.join(path, 'data_wound_seg', 'correspondence_table.xlsx')\n\n# Get list of image and mask files\ntrain_images = sorted(glob(os.path.join(train_image_dir, '*.png')))\ntrain_masks = sorted(glob(os.path.join(train_mask_dir, '*.png')))\ntest_images = sorted(glob(os.path.join(test_image_dir, '*.png')))\ntest_masks = sorted(glob(os.path.join(test_mask_dir, '*.png')))\n\nprint(\"Path to dataset files:\", path)\nprint(f\"Number of training images: {len(train_images)}\")\nprint(f\"Number of training masks: {len(train_masks)}\")\nprint(f\"Number of testing images: {len(test_images)}\")\nprint(f\"Number of testing masks: {len(test_masks)}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:03:04.967007Z\",\"iopub.execute_input\":\"2025-05-10T21:03:04.967275Z\",\"iopub.status.idle\":\"2025-05-10T21:03:05.986004Z\",\"shell.execute_reply.started\":\"2025-05-10T21:03:04.967255Z\",\"shell.execute_reply\":\"2025-05-10T21:03:05.985023Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndef display_samples(images, masks, num=5):\n    plt.figure(figsize=(12, num * 4))\n    for i in range(num):\n        img = cv2.imread(images[i])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(masks[i], cv2.IMREAD_GRAYSCALE)\n\n        plt.subplot(num, 2, 2 * i + 1)\n        plt.imshow(img)\n        plt.title('Image')\n        plt.axis('off')\n\n        plt.subplot(num, 2, 2 * i + 2)\n        plt.imshow(mask, cmap='gray')\n        plt.title('Mask')\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Display 3 samples from the training set\ndisplay_samples(train_images, train_masks, num=3)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:03:05.986926Z\",\"iopub.execute_input\":\"2025-05-10T21:03:05.987199Z\",\"iopub.status.idle\":\"2025-05-10T21:03:06.003687Z\",\"shell.execute_reply.started\":\"2025-05-10T21:03:05.987177Z\",\"shell.execute_reply\":\"2025-05-10T21:03:06.003065Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport numpy as np\nimport cv2\nfrom tensorflow.keras.utils import Sequence\n\n# Parameters\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\nBATCH_SIZE = 32\n\n# Updated Data Generator with float32 casting\nclass DataGenerator(Sequence):\n    def __init__(self, image_filenames, mask_filenames, batch_size, img_height, img_width, augment=False):\n        self.image_filenames = image_filenames\n        self.mask_filenames = mask_filenames\n        self.batch_size = batch_size\n        self.img_height = img_height\n        self.img_width = img_width\n        self.augment = augment\n\n    def __len__(self):\n        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.mask_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        images = []\n        masks = []\n\n        for i, m in zip(batch_x, batch_y):\n            # Read and preprocess image\n            img = cv2.imread(i)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (self.img_width, self.img_height))\n            img = img / 255.0  # Normalize\n            img = img.astype(np.float32)  # Cast to float32\n\n            # Read and preprocess mask\n            mask = cv2.imread(m, cv2.IMREAD_GRAYSCALE)\n            mask = cv2.resize(mask, (self.img_width, self.img_height))\n            mask = np.expand_dims(mask, axis=-1)\n            mask = mask / 255.0  # Normalize\n            mask = mask.astype(np.float32)  # Cast to float32\n\n            if self.augment:\n                # Random horizontal flip\n                if np.random.rand() > 0.5:\n                    img = np.fliplr(img)\n                    mask = np.fliplr(mask)\n\n                # Random vertical flip\n                if np.random.rand() > 0.5:\n                    img = np.flipud(img)\n                    mask = np.flipud(mask)\n\n                # Random rotation\n                angle = np.random.randint(-15, 15)\n                M = cv2.getRotationMatrix2D((self.img_width/2, self.img_height/2), angle, 1)\n                img = cv2.warpAffine(img, M, (self.img_width, self.img_height)).astype(np.float32)\n                mask = cv2.warpAffine(mask, M, (self.img_width, self.img_height)).astype(np.float32)\n\n            images.append(img)\n            masks.append(mask)\n\n        return np.array(images), np.array(masks)\n\n# Create generators\ntrain_generator = DataGenerator(train_images, train_masks, BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, augment=True)\ntest_generator = DataGenerator(test_images, test_masks, BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:03:06.004521Z\",\"iopub.execute_input\":\"2025-05-10T21:03:06.004779Z\",\"iopub.status.idle\":\"2025-05-10T21:03:06.027676Z\",\"shell.execute_reply.started\":\"2025-05-10T21:03:06.004756Z\",\"shell.execute_reply\":\"2025-05-10T21:03:06.027032Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nclass DataGenerator(Sequence):\n    def __init__(self, image_filenames, mask_filenames, batch_size, img_height, img_width, augment=False):\n        self.image_filenames = image_filenames\n        self.mask_filenames = mask_filenames\n        self.batch_size = batch_size\n        self.img_height = img_height\n        self.img_width = img_width\n        self.augment = augment\n\n    def __len__(self):\n        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.mask_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        images = []\n        masks = []\n\n        for i, m in zip(batch_x, batch_y):\n            # Read and preprocess image\n            img = cv2.imread(i)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (self.img_width, self.img_height))\n            img = img / 255.0  # Normalize\n\n            # Read and preprocess mask\n            mask = cv2.imread(m, cv2.IMREAD_GRAYSCALE)\n            mask = cv2.resize(mask, (self.img_width, self.img_height))\n            mask = np.expand_dims(mask, axis=-1)\n            mask = mask / 255.0  # Normalize\n\n            if self.augment:\n                # Random horizontal flip\n                if np.random.rand() > 0.5:\n                    img = np.fliplr(img)\n                    mask = np.fliplr(mask)\n\n                # Random vertical flip\n                if np.random.rand() > 0.5:\n                    img = np.flipud(img)\n                    mask = np.flipud(mask)\n\n                # Random rotation\n                angle = np.random.randint(-15, 15)\n                M = cv2.getRotationMatrix2D((self.img_width/2, self.img_height/2), angle, 1)\n                img = cv2.warpAffine(img, M, (self.img_width, self.img_height))\n                mask = cv2.warpAffine(mask, M, (self.img_width, self.img_height))\n\n            images.append(img)\n            masks.append(mask)\n\n        return np.array(images), np.array(masks)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:03:06.028437Z\",\"iopub.execute_input\":\"2025-05-10T21:03:06.029202Z\",\"iopub.status.idle\":\"2025-05-10T21:03:09.338588Z\",\"shell.execute_reply.started\":\"2025-05-10T21:03:06.029176Z\",\"shell.execute_reply\":\"2025-05-10T21:03:09.338034Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndef unet_model(input_size=(256, 256, 3)):\n    inputs = Input(input_size)\n\n    # Encoder\n    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n    c1 = BatchNormalization()(c1)\n    c1 = Dropout(0.1)(c1)\n    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n    p1 = MaxPooling2D((2, 2))(c1)\n\n    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n    c2 = BatchNormalization()(c2)\n    c2 = Dropout(0.1)(c2)\n    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n    p2 = MaxPooling2D((2, 2))(c2)\n\n    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n    c3 = BatchNormalization()(c3)\n    c3 = Dropout(0.2)(c3)\n    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n    p3 = MaxPooling2D((2, 2))(c3)\n\n    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n    c4 = BatchNormalization()(c4)\n    c4 = Dropout(0.2)(c4)\n    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n\n    # Bridge\n    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n    c5 = BatchNormalization()(c5)\n    c5 = Dropout(0.3)(c5)\n    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n\n    # Decoder\n    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n    c6 = BatchNormalization()(c6)\n    c6 = Dropout(0.2)(c6)\n    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n\n    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n    c7 = BatchNormalization()(c7)\n    c7 = Dropout(0.2)(c7)\n    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n\n    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n    c8 = BatchNormalization()(c8)\n    c8 = Dropout(0.1)(c8)\n    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n\n    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n    u9 = concatenate([u9, c1])\n    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n    c9 = BatchNormalization()(c9)\n    c9 = Dropout(0.1)(c9)\n    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n\n    # Output\n    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n\n    model = Model(inputs=[inputs], outputs=[outputs])\n    return model\n\n# Instantiate the model\nmodel = unet_model()\nmodel.summary()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:03:09.339364Z\",\"iopub.execute_input\":\"2025-05-10T21:03:09.339733Z\",\"iopub.status.idle\":\"2025-05-10T21:03:09.345684Z\",\"shell.execute_reply.started\":\"2025-05-10T21:03:09.339707Z\",\"shell.execute_reply\":\"2025-05-10T21:03:09.345162Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport tensorflow.keras.backend as K\n\ndef dice_coefficient(y_true, y_pred, smooth=1):\n    y_true_f = K.cast(K.flatten(y_true), 'float32')\n    y_pred_f = K.cast(K.flatten(y_pred), 'float32')\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef iou_metric(y_true, y_pred, smooth=1):\n    y_true_f = K.cast(K.flatten(y_true), 'float32')\n    y_pred_f = K.cast(K.flatten(y_pred), 'float32')\n    intersection = K.sum(y_true_f * y_pred_f)\n    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection\n    return (intersection + smooth) / (union + smooth)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:03:09.346349Z\",\"iopub.execute_input\":\"2025-05-10T21:03:09.346592Z\",\"iopub.status.idle\":\"2025-05-10T21:03:09.372523Z\",\"shell.execute_reply.started\":\"2025-05-10T21:03:09.346576Z\",\"shell.execute_reply\":\"2025-05-10T21:03:09.371855Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Compile the model with additional metrics\nmodel.compile(optimizer=Adam(learning_rate=1e-4),\n              loss='binary_crossentropy',\n              metrics=['accuracy', dice_coefficient, iou_metric])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:36:01.363986Z\",\"iopub.execute_input\":\"2025-05-10T21:36:01.364641Z\",\"iopub.status.idle\":\"2025-05-10T21:47:46.064353Z\",\"shell.execute_reply.started\":\"2025-05-10T21:36:01.364613Z\",\"shell.execute_reply\":\"2025-05-10T21:47:46.063474Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n# Updated Callbacks with .keras extension\ncheckpoint = ModelCheckpoint('unet_wound_segmentation_best.keras',\n                             monitor='val_loss',\n                             save_best_only=True,\n                             verbose=1,\n                             mode='min')\n\nearlystop = EarlyStopping(monitor='val_loss',\n                          patience=15,\n                          verbose=1,\n                          restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.5,\n                              patience=7,\n                              verbose=1,\n                              mode='min',\n                              min_lr=1e-6)\ncallbacks_list = [checkpoint, earlystop, reduce_lr]\n\n# Compile the model with updated metrics\nmodel.compile(optimizer=Adam(learning_rate=1e-4),\n              loss='binary_crossentropy',\n              metrics=['accuracy', dice_coefficient, iou_metric])\n\n# Training the model\nhistory = model.fit(\n    train_generator,\n    validation_data=test_generator,\n    epochs=5,  # Increased epochs to allow for more training if needed\n    callbacks=callbacks_list,\n    verbose=1\n)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T22:13:12.759639Z\",\"iopub.execute_input\":\"2025-05-10T22:13:12.760274Z\",\"iopub.status.idle\":\"2025-05-10T22:13:12.766853Z\",\"shell.execute_reply.started\":\"2025-05-10T22:13:12.760251Z\",\"shell.execute_reply\":\"2025-05-10T22:13:12.765897Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndef visualize_predictions(model, generator, num=3):\n    import matplotlib.pyplot as plt\n\n    # Get a batch of data from the generator\n    images, masks = generator.__getitem__(0)  # Get the first batch\n    predictions = model.predict(images)\n\n    plt.figure(figsize=(12, num * 4))\n\n    for i in range(num):\n        img = images[i]\n        true_mask = masks[i].squeeze()\n        pred_mask = predictions[i].squeeze()\n\n        # Binarize predicted mask (optional)\n        pred_mask_bin = (pred_mask > 0.5).astype(np.uint8)\n\n        # Original Image\n        plt.subplot(num, 3, i * 3 + 1)\n        plt.imshow(img)\n        plt.title('Image')\n        plt.axis('off')\n\n        # Ground Truth Mask\n        plt.subplot(num, 3, i * 3 + 2)\n        plt.imshow(true_mask, cmap='gray')\n        plt.title('Ground Truth Mask')\n        plt.axis('off')\n\n        # Predicted Mask\n        plt.subplot(num, 3, i * 3 + 3)\n        plt.imshow(pred_mask_bin, cmap='gray')\n        plt.title('Predicted Mask')\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T22:13:15.521696Z\",\"iopub.execute_input\":\"2025-05-10T22:13:15.522070Z\",\"iopub.status.idle\":\"2025-05-10T22:13:15.530681Z\",\"shell.execute_reply.started\":\"2025-05-10T22:13:15.522047Z\",\"shell.execute_reply\":\"2025-05-10T22:13:15.529612Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nvisualize_predictions(model, test_generator, num=5)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# Try#\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:48:22.502787Z\",\"iopub.execute_input\":\"2025-05-10T21:48:22.503073Z\",\"iopub.status.idle\":\"2025-05-10T21:48:22.511926Z\",\"shell.execute_reply.started\":\"2025-05-10T21:48:22.503055Z\",\"shell.execute_reply\":\"2025-05-10T21:48:22.511185Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport cv2\nimport numpy as np\n\ndef detect_white_square_and_measure(image, mask, square_cm=2.0):\n    # 1. Convert image to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # 2. Threshold for white color (tune thresholds if needed)\n    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n\n    # 3. Find contours in threshold image\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    pixels_per_cm = None\n    for cnt in contours:\n        approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n        area = cv2.contourArea(cnt)\n        if len(approx) == 4 and area > 100:  # a square with reasonable size\n            x, y, w, h = cv2.boundingRect(cnt)\n            if abs(w - h) < 10:  # width ≈ height\n                square_pixel_size = (w + h) / 2.0\n                pixels_per_cm = square_pixel_size / square_cm\n                break  # use the first valid square\n\n    if pixels_per_cm is None:\n        raise ValueError(\"White square reference not detected.\")\n\n    # 4. Find wound contour in mask\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if len(contours) == 0:\n        raise ValueError(\"No wound segmentation found.\")\n\n    wound_cnt = max(contours, key=cv2.contourArea)\n    x, y, w, h = cv2.boundingRect(wound_cnt)\n    wound_area_px = cv2.contourArea(wound_cnt)\n\n    # 5. Convert to cm\n    wound_width_cm = w / pixels_per_cm\n    wound_height_cm = h / pixels_per_cm\n    wound_area_cm2 = wound_area_px / (pixels_per_cm ** 2)\n\n    return {\n        \"width_cm\": round(wound_width_cm, 2),\n        \"height_cm\": round(wound_height_cm, 2),\n        \"area_cm2\": round(wound_area_cm2, 2),\n        \"pixels_per_cm\": round(pixels_per_cm, 2)\n    }\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T21:59:33.752350Z\",\"iopub.execute_input\":\"2025-05-10T21:59:33.752668Z\",\"iopub.status.idle\":\"2025-05-10T21:59:41.415861Z\",\"shell.execute_reply.started\":\"2025-05-10T21:59:33.752646Z\",\"shell.execute_reply\":\"2025-05-10T21:59:41.415140Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport cv2\nimport numpy as np\nfrom tensorflow.keras.models import load_model\nimport matplotlib.pyplot as plt\n\n# Load your trained model\nmodel = load_model(\"unet_wound_segmentation_best.keras\", compile=False)\n\n# Load and preprocess image\nimage_path = \"/kaggle/input/screenshot-2025-05-11-005723/Screenshot 2025-05-11 005723.jpg\"\noriginal = cv2.imread(image_path)\nresized = cv2.resize(original, (256, 256))  # Use the same size used in training\ninput_image = resized / 255.0\ninput_image = np.expand_dims(input_image, axis=0)\n\n# Predict mask\npred_mask = model.predict(input_image)[0]\nbinary_mask = (pred_mask > 0.5).astype(np.uint8)  # Threshold the output\nbinary_mask = binary_mask[:, :, 0] * 255  # Convert to 0-255 format\n\n# Resize back to original image size\nfinal_mask = cv2.resize(binary_mask, (original.shape[1], original.shape[0]), interpolation=cv2.INTER_NEAREST)\n\n# Save or visualize the result\ncv2.imwrite(\"wound_mask.png\", final_mask)\n\n# Optional: show image and mask\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.title(\"Original Image\")\nplt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.title(\"Predicted Mask\")\nplt.imshow(final_mask, cmap=\"gray\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T22:12:52.389411Z\",\"iopub.execute_input\":\"2025-05-10T22:12:52.389700Z\",\"execution_failed\":\"2025-05-10T22:12:54.899Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport cv2\nimport numpy as np\n\n# Load the image\nimage = cv2.imread('/kaggle/input/screenshot-2025-05-11-005723/Screenshot 2025-05-11 005723.jpg')\n\n# Convert image to HSV\nhsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n# Define range for blue color in HSV (you may fine-tune if needed)\nlower_blue = np.array([100, 100, 50])\nupper_blue = np.array([140, 255, 255])\n\n# Create a mask for the blue reference square\nmask = cv2.inRange(hsv, lower_blue, upper_blue)\n\n# Find contours in the mask\ncontours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n# Sort contours by area and get the largest one (assuming it's the blue square)\ncontours = sorted(contours, key=cv2.contourArea, reverse=True)\n\n# Draw the largest contour (blue square)\nblue_square = contours[0]\ncv2.drawContours(image, [blue_square], -1, (0, 255, 0), 3)  # Green contour for visibility\n\n# Calculate the dimensions of the blue square\nx, y, w, h = cv2.boundingRect(blue_square)\n\n# Assuming the blue square represents a known size (e.g., 2 cm x 2 cm), calculate the pixel-to-cm ratio\npixel_to_cm_ratio = 2 / max(w, h)  # Use the largest side to calculate the ratio\n\n# Get the width and height of the wound in pixels\nwound_width, wound_height = 200, 150  # Example dimensions of the wound, replace with your own detection\n\n# Convert wound dimensions to cm using the pixel-to-cm ratio\nwound_width_cm = wound_width * pixel_to_cm_ratio\nwound_height_cm = wound_height * pixel_to_cm_ratio\n\n# Show the result\ncv2.imshow('Detected Blue Square', image)\nprint(f\"Wound dimensions: {wound_width_cm:.2f} cm x {wound_height_cm:.2f} cm\")\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T22:12:44.917077Z\",\"iopub.execute_input\":\"2025-05-10T22:12:44.917907Z\",\"iopub.status.idle\":\"2025-05-10T22:12:44.928949Z\",\"shell.execute_reply.started\":\"2025-05-10T22:12:44.917879Z\",\"shell.execute_reply\":\"2025-05-10T22:12:44.927812Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport cv2\n\n\n# Convert image to HSV\nhsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n# Define range for blue color in HSV (you may fine-tune if needed)\nlower_blue = np.array([100, 100, 50])\nupper_blue = np.array([130, 255, 255])\n\n# Create a mask for the blue color\nref_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n\n# Find contours in the mask\ncontours, _ = cv2.findContours(ref_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n# Find the largest contour (assuming it's the square)\nreference_contour = max(contours, key=cv2.contourArea)\n\n# Get bounding box of the reference object\nx, y, w, h = cv2.boundingRect(reference_contour)\n\n# Calculate pixels per cm\nref_pixels = np.mean([w, h])\nref_cm = 2  # Known size of the square (2 cm)\npixels_per_cm = ref_pixels / ref_cm\n\nprint(f\"Reference size in pixels: {w}x{h}\")\nprint(f\"Pixels per cm: {pixels_per_cm:.2f}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-10T22:09:01.764902Z\",\"iopub.execute_input\":\"2025-05-10T22:09:01.765260Z\",\"execution_failed\":\"2025-05-10T22:09:03.953Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport cv2\nimport numpy as np\n\n# --- Load original image and predicted mask ---\nimage = cv2.imread('/kaggle/input/screenshot-2025-05-11-005723/Screenshot 2025-05-11 005723.jpg')\nmask = cv2.imread('/kaggle/working/wound_mask.png', 0)  # Grayscale\n\n# --- Detect reference object (white square of known size, e.g. 2cm x 2cm) ---\n\n# Convert to HSV for color detection (you can use BGR if needed)\nhsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n# Threshold white (tune these values as needed)\nlower_white = np.array([0, 0, 200])\nupper_white = np.array([180, 30, 255])\nref_mask = cv2.inRange(hsv, lower_white, upper_white)\n\n# Find contours\ncontours, _ = cv2.findContours(ref_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\nreference_contour = max(contours, key=cv2.contourArea)\n\n# Get bounding box of reference object\nx, y, w, h = cv2.boundingRect(reference_contour)\nref_pixels = np.mean([w, h])\nref_cm = 2  # If your square is 2 cm\npixels_per_cm = ref_pixels / ref_cm\n\nprint(f\"Reference object size in pixels: {w}x{h}\")\nprint(f\"Pixels per cm: {pixels_per_cm:.2f}\")\n\n# --- Measure wound area from mask ---\n\n# Find contours of wound\nwound_contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\nwound_contour = max(wound_contours, key=cv2.contourArea)\n\n# Bounding box of wound\nx, y, w, h = cv2.boundingRect(wound_contour)\n\n# Convert to cm\nwound_width_cm = w / pixels_per_cm\nwound_height_cm = h / pixels_per_cm\nwound_area_cm2 = cv2.contourArea(wound_contour) / (pixels_per_cm ** 2)\n\nprint(f\"\\nWound width: {wound_width_cm:.2f} cm\")\nprint(f\"Wound height: {wound_height_cm:.2f} cm\")\nprint(f\"Wound area: {wound_area_cm2:.2f} cm²\")\n\n# Optional: draw results\noutput = image.copy()\ncv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 2)\ncv2.putText(output, f\"{wound_width_cm:.1f}x{wound_height_cm:.1f} cm\", (x, y - 10),\n            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\ncv2.imshow(\"Measured Wound\", output)\ncv2.waitKey(0)\ncv2.destroyAllWindows()","metadata":{"_uuid":"3f658223-f2a8-4372-b039-3f1389dddfc6","_cell_guid":"46de2ebd-8414-464c-9d66-f0188c3b5e97","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}