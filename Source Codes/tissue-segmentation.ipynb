{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11831422,"sourceType":"datasetVersion","datasetId":7432806},{"sourceId":11959863,"sourceType":"datasetVersion","datasetId":7519503}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Suupervised training** <br>\nLabels 1 and 3 of some images have been augmented, so that the dataset contains more images that have labels 1 and 3.","metadata":{"_uuid":"f594862e-9b24-4039-9fba-015434fee859","_cell_guid":"25321385-10af-40ac-883e-d200ab11e106","trusted":true,"collapsed":false,"id":"8XKGaVdvIuwq","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nBASE_PATH = '/kaggle/input/dfuc2020/GRAD PROJECT/DFUC2020'\nDATA_PATH = BASE_PATH\n\ntrain_images_dir = os.path.join(DATA_PATH, 'train', 'Images', 'TrainVal')\ntrain_masks_dir  = os.path.join(DATA_PATH, 'train', 'Annotations', 'TrainVal')\ntest_images_dir  = os.path.join(DATA_PATH, 'test', 'Images', 'Test')\ntest_masks_dir   = os.path.join(DATA_PATH, 'test', 'Annotations', 'Test')\n\ndir_txt = os.path.join(DATA_PATH, 'train')","metadata":{"_uuid":"af120999-e544-4478-93c9-b6bdd0c60df6","_cell_guid":"8333a62f-b673-446f-b59d-b7950ffc4832","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-26T17:31:31.414811Z","iopub.execute_input":"2025-05-26T17:31:31.415241Z","iopub.status.idle":"2025-05-26T17:31:31.425359Z","shell.execute_reply.started":"2025-05-26T17:31:31.415184Z","shell.execute_reply":"2025-05-26T17:31:31.424676Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q segmentation-models-pytorch albumentations\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset\nimport albumentations as albu\nimport cv2\nimport numpy as np\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.utils import metrics, losses, base\nimport random\nimport matplotlib.pyplot as plt\nimport os\nfrom copy import deepcopy\nfrom datetime import datetime\nimport torch.nn.functional as F\n\n%matplotlib inline","metadata":{"_uuid":"23baa289-8a42-46c2-9960-f87608bea5f2","_cell_guid":"5bca932c-a8e8-4ea5-ae7e-badce9247407","trusted":true,"id":"wWvUuJ7xZ17E","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-05-26T17:31:31.426496Z","iopub.execute_input":"2025-05-26T17:31:31.426727Z","iopub.status.idle":"2025-05-26T17:33:16.781546Z","shell.execute_reply.started":"2025-05-26T17:31:31.426710Z","shell.execute_reply":"2025-05-26T17:33:16.780736Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataloader","metadata":{"_uuid":"4d716563-20ab-44af-8db7-e0687c8ca071","_cell_guid":"edd70b8b-93bc-48b0-a815-3ca9f723aa97","trusted":true,"collapsed":false,"id":"u1TTozpn-D8o","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class Dataset(BaseDataset):\n    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n\n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        augmentation (albumentations.Compose): data transfromation pipeline\n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing\n            (e.g. noralization, shape manipulation, etc.)\n\n    \"\"\"\n\n    def __init__(\n            self,\n            list_IDs,\n            images_dir,\n            masks_dir,\n            augmentation=None,\n            preprocessing=None,\n            to_categorical:bool=False,\n            resize=(False, (256, 256)), # To resize, the first value has to be True\n            n_classes:int=6,\n            default_img=None,\n            default_mask=None,\n    ):\n        self.ids = list_IDs\n        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.to_categorical = to_categorical\n        self.resize = resize\n        self.n_classes = n_classes\n        self.default_img = default_img\n        self.default_mask = default_mask\n\n    def __getitem__(self, i):\n        try:\n              # read data\n              image = cv2.imread(self.images_fps[i])\n              image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n              mask = cv2.imread(self.masks_fps[i], 0)     # ----------------- pay attention ------------------ #\n        except Exception as e:\n            print(\"********** Error occured loading default image and mask. *********\")\n            image = self.default_img\n            mask = self.default_mask\n\n        if self.resize[0]:\n            image = cv2.resize(image, self.resize[1], interpolation=cv2.INTER_NEAREST)\n            mask = cv2.resize(mask, self.resize[1], interpolation=cv2.INTER_NEAREST)\n\n        mask = np.expand_dims(mask, axis=-1)  # adding channel axis # ----------------- pay attention ------------------ #\n\n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n\n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n\n        if self.to_categorical:\n            mask = torch.from_numpy(mask)\n            mask = F.one_hot(mask.long(), num_classes=self.n_classes)\n            mask = mask.type(torch.float32)\n            mask = mask.numpy()\n            mask = np.squeeze(mask)\n\n            mask = np.moveaxis(mask, -1, 0)\n\n        return image, mask\n\n    def __len__(self):\n        return len(self.ids)","metadata":{"_uuid":"084c3b61-3fdb-4073-9ce5-04c7f127981e","_cell_guid":"1f063580-a27b-41d0-9313-62e611c18430","trusted":true,"collapsed":false,"id":"nx_iN5gGarSj","execution":{"iopub.status.busy":"2025-05-26T17:33:16.782908Z","iopub.execute_input":"2025-05-26T17:33:16.783361Z","iopub.status.idle":"2025-05-26T17:33:16.792581Z","shell.execute_reply.started":"2025-05-26T17:33:16.783332Z","shell.execute_reply":"2025-05-26T17:33:16.791778Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Augmentation","metadata":{"_uuid":"d7b8628f-7754-4304-893d-c1d28a033c11","_cell_guid":"b5e8d414-5014-4784-a489-5d01c003c746","trusted":true,"collapsed":false,"id":"yB8RKETkALtF","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n\n        albu.OneOf(\n            [\n                albu.HorizontalFlip(p=0.5),\n                albu.VerticalFlip(p=0.5),\n            ],\n            p=0.8,\n        ),\n\n        albu.OneOf(\n            [\n                albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0, p=0.1, border_mode=0), # scale only\n                albu.ShiftScaleRotate(scale_limit=0, rotate_limit=30, shift_limit=0, p=0.1, border_mode=0), # rotate only\n                albu.ShiftScaleRotate(scale_limit=0, rotate_limit=0, shift_limit=0.1, p=0.6, border_mode=0), # shift only\n                albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=30, shift_limit=0.1, p=0.2, border_mode=0), # affine transform\n            ],\n            p=0.9,\n        ),\n\n        albu.OneOf(\n            [\n                albu.Perspective(p=0.2),\n                albu.GaussNoise(p=0.2),\n                albu.Sharpen(p=0.2),\n                albu.Blur(blur_limit=3, p=0.2),\n                albu.MotionBlur(blur_limit=3, p=0.2),\n            ],\n            p=0.5,\n        ),\n\n        albu.OneOf(\n            [\n                albu.CLAHE(p=0.25),\n                albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.25),\n                albu.RandomGamma(p=0.25),\n                albu.HueSaturationValue(p=0.25),\n            ],\n            p=0.3,\n        ),\n\n    ]\n\n    return albu.Compose(train_transform, p=0.9) # 90% augmentation probability\n\n\ndef get_validation_augmentation():\n    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n    test_transform = [\n        # albu.PadIfNeeded(512, 512)\n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n\n    Args:\n        preprocessing_fn (callbale): data normalization function\n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n\n    \"\"\"\n\n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","metadata":{"_uuid":"c4e77fb0-1c5e-42ed-aec0-3c73f3e0c117","_cell_guid":"2eddab1d-19e3-4052-97ba-25b3894a1afe","trusted":true,"collapsed":false,"id":"T-ltFwaGAK3c","execution":{"iopub.status.busy":"2025-05-26T17:33:16.793264Z","iopub.execute_input":"2025-05-26T17:33:16.793463Z","iopub.status.idle":"2025-05-26T17:33:16.819850Z","shell.execute_reply.started":"2025-05-26T17:33:16.793447Z","shell.execute_reply":"2025-05-26T17:33:16.819147Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Parameters","metadata":{"_uuid":"13e4f08c-a3f2-4eb5-8547-45f4a41f64a9","_cell_guid":"dd2c84ef-7cdb-4126-a650-c3b4e0573ca5","trusted":true,"collapsed":false,"id":"Z9xxDHfp_yFO","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Parameters\nBASE_MODEL = 'MiT+pscse'\nENCODER = 'mit_b3'\nENCODER_WEIGHTS = 'imagenet'\nBATCH_SIZE = 16\nn_classes = 9\nACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nLR = 0.0001 # learning rate\nEPOCHS = 500\nWEIGHT_DECAY = 1e-5\nSAVE_WEIGHTS_ONLY = True\nRESIZE = (False, (256,256)) # if resize needed\nTO_CATEGORICAL = True\nSAVE_BEST_MODEL = True\nSAVE_LAST_MODEL = False\n\nPERIOD = 10 # periodically save checkpoints\nRAW_PREDICTION = False # if true, then stores raw predictions (i.e. before applying threshold)\nRETRAIN = False\n\n# For early stopping\nEARLY_STOP = True # True to activate early stopping\nPATIENCE = 50 # for early stopping\n\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context","metadata":{"_uuid":"d35977d5-1242-42c2-8125-86c6183fc8e9","_cell_guid":"63f3f940-eef2-4573-b3ba-deaa1874dab3","trusted":true,"collapsed":false,"id":"Yq5o1vzBYxN0","execution":{"iopub.status.busy":"2025-05-26T17:33:16.821559Z","iopub.execute_input":"2025-05-26T17:33:16.822011Z","iopub.status.idle":"2025-05-26T17:33:16.945515Z","shell.execute_reply.started":"2025-05-26T17:33:16.821993Z","shell.execute_reply":"2025-05-26T17:33:16.944814Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Helper function: save a model","metadata":{"_uuid":"6128a975-6726-4aad-bdc9-e44a0fdb2889","_cell_guid":"bdc920cc-48a0-4e50-b92b-795ef0db51b4","trusted":true,"collapsed":false,"id":"aZUDgFCkKPlW","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def save(model_path, epoch, model_state_dict, optimizer_state_dict):\n\n    state = {\n        'epoch': epoch + 1,\n        'state_dict': deepcopy(model_state_dict),\n        'optimizer': deepcopy(optimizer_state_dict),\n        }\n\n    torch.save(state, model_path)","metadata":{"_uuid":"155f9222-c064-477c-ba07-eb40e41a6e98","_cell_guid":"897a22a7-f057-4394-bfc6-7e1efcaaf9d6","trusted":true,"collapsed":false,"id":"daug57SuKNIW","execution":{"iopub.status.busy":"2025-05-26T17:33:16.946258Z","iopub.execute_input":"2025-05-26T17:33:16.946450Z","iopub.status.idle":"2025-05-26T17:33:16.964129Z","shell.execute_reply.started":"2025-05-26T17:33:16.946435Z","shell.execute_reply":"2025-05-26T17:33:16.963520Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loss, optimizer, metrics, and callbacks","metadata":{"_uuid":"b4640f9a-a6d8-42ea-811b-131695b2b4e9","_cell_guid":"f7cc4717-1473-4b1f-9c25-884e43f2158e","trusted":true,"collapsed":false,"id":"7qcadi38Ac5H","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Loss function\ndice_loss = losses.DiceLoss()\n##focal_loss = losses.FocalLoss()\ntotal_loss = dice_loss\n\n# Metrics\nmetrics = [\n    metrics.IoU(threshold=0.5),\n    metrics.Fscore(threshold=0.5),\n]","metadata":{"_uuid":"b281699e-085f-42bb-aa18-1b5f40ff4be0","_cell_guid":"67cc0136-a26f-43e2-9c97-4a373e03dd36","trusted":true,"collapsed":false,"id":"26g3jFOZyWmg","execution":{"iopub.status.busy":"2025-05-26T17:33:16.964806Z","iopub.execute_input":"2025-05-26T17:33:16.965034Z","iopub.status.idle":"2025-05-26T17:33:16.981140Z","shell.execute_reply.started":"2025-05-26T17:33:16.965017Z","shell.execute_reply":"2025-05-26T17:33:16.980576Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### Model Run","metadata":{"_uuid":"26d3a7e9-36ac-4d0a-8c95-77473b79e271","_cell_guid":"99e854dc-1078-47e7-94f8-e4326f9485c0","trusted":true,"collapsed":false,"id":"Mp9ohG-nyZzg","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Create a function to read names from a text file, and add extensions\ndef read_names(txt_file, ext=\".png\"):\n  with open(txt_file, \"r\") as f: names = f.readlines()\n\n  names = [name.strip(\"\\n\") for name in names] # remove newline\n\n  # Names are without extensions. So, add extensions\n  names = [name + ext for name in names]\n\n  return names","metadata":{"_uuid":"51427dcb-f50b-46bb-8a63-fac77f9dd53c","_cell_guid":"cb0c2912-de7a-4aec-89cc-3866dd384555","trusted":true,"collapsed":false,"id":"nr8W1S3S7EhM","execution":{"iopub.status.busy":"2025-05-26T17:33:16.981728Z","iopub.execute_input":"2025-05-26T17:33:16.982003Z","iopub.status.idle":"2025-05-26T17:33:16.997581Z","shell.execute_reply.started":"2025-05-26T17:33:16.981974Z","shell.execute_reply":"2025-05-26T17:33:16.997062Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{"_uuid":"908109aa-83e9-433c-b8af-fcdd665c096f","_cell_guid":"05660ae3-39f4-45b1-884b-f10763ad88b6","trusted":true,"collapsed":false,"id":"2RJOTrSrfvDA","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nsave_dir_pred_root = '/kaggle/working/predictions/'\nos.makedirs(save_dir_pred_root, exist_ok = True)\n\naux_params=dict(\n    classes=n_classes,\n    activation=ACTIVATION,\n    dropout=0.1, # dropout ratio, default is None\n)\n\n# create segmentation model with pretrained encoder\nmodel = smp.Unet(\n    encoder_name=ENCODER,\n    encoder_weights=ENCODER_WEIGHTS,\n    # aux_params=aux_params,\n    classes=n_classes,\n    activation=ACTIVATION,\n    decoder_attention_type='scse',\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n\nmodel.to(DEVICE)\n\n# Optimizer\noptimizer = torch.optim.Adam([\n    dict(params=model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY),\n])\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                              factor=0.1,\n                              mode='min',\n                              patience=10,\n                              min_lr=0.00001,\n                              verbose=True,\n                              )\n\nseed = random.randint(0, 5000)\n\nprint(f'seed: {seed}')\n\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\n\nx_train_dir = train_images_dir\ny_train_dir = train_masks_dir\nx_valid_dir = train_images_dir       # If you have separate val set, set accordingly\ny_valid_dir = train_masks_dir\nx_test_dir  = test_images_dir\ny_test_dir  = test_masks_dir\n\ndir_txt = os.path.join(DATA_PATH, 'train')\n    \n\nlist_IDs_train = read_names(os.path.join(dir_txt, 'labeled_train_names.txt'), ext='.png')\nlist_IDs_val = read_names(os.path.join(dir_txt, 'labeled_val_names.txt'), ext='.png')\nlist_IDs_test = read_names(os.path.join(dir_txt, 'test_names.txt'), ext='.png')\n\nrandom.seed(seed) # seed for random number generator\nrandom.shuffle(list_IDs_train) # shuffle train names\n\nprint('No. of training images: ', len(list_IDs_train))\nprint('No. of validation images: ', len(list_IDs_val))\nprint('No. of test images: ', len(list_IDs_test))\n\n# Create a unique model name\nmodel_name = BASE_MODEL + '_padded_aug_' + ENCODER + '_sup_' + datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\nprint(model_name)\n\n# Default images\nDEFAULT_IMG_TRAIN = cv2.imread(os.path.join(x_train_dir, list_IDs_train[0]))[:,:,::-1]\nDEFAULT_MASK_TRAIN = cv2.imread(os.path.join(y_train_dir, list_IDs_train[0]), 0)\nDEFAULT_IMG_VAL = cv2.imread(os.path.join(x_valid_dir, list_IDs_val[0]))[:,:,::-1]\nDEFAULT_MASK_VAL = cv2.imread(os.path.join(y_valid_dir, list_IDs_val[0]), 0)\n\ncheckpoint_loc = '/kaggle/working/checkpoints/' + model_name\nif not os.path.exists(checkpoint_loc): os.makedirs(checkpoint_loc)\n\n\n# if SAVE_BEST_MODEL_ONLY: checkpoint_path = os.path.join(checkpoint_loc, 'best_model.pth')\n# else: checkpoint_path = os.path.join(checkpoint_loc, \"cp-{epoch:04d}.pth\")\n\n# Dataloader ===================================================================\ntrain_dataset = Dataset(\n    list_IDs_train,\n    x_train_dir,\n    y_train_dir,\n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    to_categorical=TO_CATEGORICAL,\n    resize=(RESIZE),\n    n_classes=n_classes,\n    default_img=DEFAULT_IMG_TRAIN,\n    default_mask=DEFAULT_MASK_TRAIN,\n)\n\nvalid_dataset = Dataset(\n    list_IDs_val,\n    x_valid_dir,\n    y_valid_dir,\n    augmentation=get_validation_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    resize=(RESIZE),\n    to_categorical=TO_CATEGORICAL,\n    n_classes=n_classes,\n    default_img=DEFAULT_IMG_VAL,\n    default_mask=DEFAULT_MASK_VAL,\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=6)\n\n# create epoch runners =========================================================\n# it is a simple loop of iterating over dataloader`s samples\ntrain_epoch = smp.utils.train.TrainEpoch(\n    model,\n    loss=total_loss,\n    metrics=metrics,\n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model,\n    loss=total_loss,\n    metrics=metrics,\n    device=DEVICE,\n    verbose=True,\n)\n\n# Train ========================================================================\n# train model for N epochs\nbest_viou = 0.0\nbest_vloss = 1_000_000.\nsave_model = False # Initially start with False\ncnt_patience = 0\n\nstore_train_loss, store_val_loss = [], []\nstore_train_iou, store_val_iou = [], []\nstore_train_dice, store_val_dice = [], []\n\nfor epoch in range(EPOCHS):\n\n    print('\\nEpoch: {}'.format(epoch))\n    train_logs = train_epoch.run(train_loader)\n    valid_logs = valid_epoch.run(valid_loader)\n\n    # Store losses and metrics\n    train_loss_key = list(train_logs.keys())[0] # first key is for loss\n    val_loss_key = list(valid_logs.keys())[0] # first key is for loss\n\n    store_train_loss.append(train_logs[train_loss_key])\n    store_val_loss.append(valid_logs[val_loss_key])\n    store_train_iou.append(train_logs[\"iou_score\"])\n    store_val_iou.append(valid_logs[\"iou_score\"])\n    store_train_dice.append(train_logs[\"fscore\"])\n    store_val_dice.append(valid_logs[\"fscore\"])\n\n    # Track best performance, and save the model's state\n    if  best_vloss > valid_logs[val_loss_key]:\n        best_vloss = valid_logs[val_loss_key]\n        print(f'Validation loss reduced. Saving the model at epoch: {epoch:04d}')\n        cnt_patience = 0 # reset patience\n        best_model_epoch = epoch\n        save_model = True\n\n    # Compare iou score\n    elif best_viou < valid_logs['iou_score']:\n        best_viou = valid_logs['iou_score']\n        print(f'Validation IoU increased. Saving the model at epoch: {epoch:04d}.')\n        cnt_patience = 0 # reset patience\n        best_model_epoch = epoch\n        save_model = True\n\n    else: cnt_patience += 1\n\n    # Learning rate scheduler\n    scheduler.step(valid_logs[sorted(valid_logs.keys())[0]]) # monitor validation loss\n\n    # Save the model\n    if save_model:\n        save(os.path.join(checkpoint_loc, 'best_model' + '.pth'),\n            epoch+1, model.state_dict(), optimizer.state_dict())\n        save_model = False\n\n    # Early stopping\n    if EARLY_STOP and cnt_patience >= PATIENCE:\n        print(f\"Early stopping at epoch: {epoch:04d}\")\n        break\n\n    # Periodic checkpoint save\n    if not SAVE_BEST_MODEL:\n        if (epoch+1) % PERIOD == 0:\n            save(os.path.join(checkpoint_loc, f\"cp-{epoch+1:04d}.pth\"),\n                epoch+1, model.state_dict(), optimizer.state_dict())\n            print(f'Checkpoint saved for epoch {epoch:04d}')\n\nif not EARLY_STOP and SAVE_LAST_MODEL:\n    print('Saving last model')\n    save(os.path.join(checkpoint_loc, 'last_model' + '.pth'),\n        epoch+1, model.state_dict(), optimizer.state_dict())\n\nprint(best_model_epoch)","metadata":{"_uuid":"af1956ff-11e5-474e-8661-de419dc3f125","_cell_guid":"b776a286-3d6a-4eb4-ae3a-66e48a08947d","trusted":true,"collapsed":false,"id":"MwRA6u_4ZvyE","execution":{"iopub.status.busy":"2025-05-26T17:33:16.998260Z","iopub.execute_input":"2025-05-26T17:33:16.998599Z","iopub.status.idle":"2025-05-26T18:08:08.136242Z","shell.execute_reply.started":"2025-05-26T17:33:16.998572Z","shell.execute_reply":"2025-05-26T18:08:08.135440Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plot loss curves","metadata":{"_uuid":"868cce0e-bd52-4ec9-9d3e-c4e6f0fd27d8","_cell_guid":"b252d287-f37f-4d28-8721-3c6dcbef0e75","trusted":true,"collapsed":false,"id":"lk6wLszofmdv","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Plot loss curves =============================================================\nfig, ax = plt.subplots(1,3, figsize=(12, 3))\n\nax[0].plot(store_train_loss, 'r')\nax[0].plot(store_val_loss, 'b')\nax[0].set_title('Loss curve')\nax[0].legend(['training', 'validation'])\n\nax[1].plot(store_train_iou, 'r')\nax[1].plot(store_val_iou, 'b')\nax[1].set_title('IoU curve')\nax[1].legend(['training', 'validation'])\n\nax[2].plot(store_train_iou, 'r')\nax[2].plot(store_val_iou, 'b')\nax[2].set_title('Dice curve')\nax[2].legend(['training', 'validation'])\n\nfig.tight_layout()\n\nsave_fig_dir = \"/content/drive/MyDrive/Wound_tissue_segmentation/plots/\"\nif not os.path.exists(save_fig_dir): os.makedirs(save_fig_dir)\n\nfig.savefig(os.path.join(save_fig_dir, model_name + '.png'))","metadata":{"_uuid":"b3588e99-f806-42d2-ab27-7472e5b1e00a","_cell_guid":"a1f13d57-4753-4d27-b62f-5211dabadffd","trusted":true,"collapsed":false,"id":"D6U9yDYcflOe","execution":{"iopub.status.busy":"2025-05-26T18:08:08.137340Z","iopub.execute_input":"2025-05-26T18:08:08.137719Z","iopub.status.idle":"2025-05-26T18:08:09.218057Z","shell.execute_reply.started":"2025-05-26T18:08:08.137691Z","shell.execute_reply":"2025-05-26T18:08:09.217321Z"},"jupyter":{"source_hidden":true,"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference","metadata":{"_uuid":"1ae82179-c155-45e3-85b1-0e563cd70e9b","_cell_guid":"05dec8fd-d3b1-4ebc-a2eb-57a4af2e12c9","trusted":true,"collapsed":false,"id":"R81fmCqBfaB_","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nprint(os.listdir('/kaggle/working/checkpoints/MiT+pscse_padded_aug_mit_b3_sup_2025-05-26_15-04-55'))","metadata":{"_uuid":"727aa100-05ca-4fb7-8c5e-55f46b11435c","_cell_guid":"0fadbd55-ab4b-4b0e-8e36-89c59f1e1135","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-26T18:08:09.220254Z","iopub.execute_input":"2025-05-26T18:08:09.220472Z","iopub.status.idle":"2025-05-26T18:08:09.271747Z","shell.execute_reply.started":"2025-05-26T18:08:09.220455Z","shell.execute_reply":"2025-05-26T18:08:09.270895Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fix test masks directory for capital \"T\" in 'Test'\ny_test_dir = y_test_dir.replace('/test/Annotations/test', '/test/Annotations/Test')\n\n# =================================== Inference ================================\n# Load model====================================================================\ncheckpoint_loc = '/kaggle/working/checkpoints/MiT+pscse_padded_aug_mit_b3_sup_2025-05-26_15-04-55'\ncheckpoint = torch.load(os.path.join(checkpoint_loc, 'best_model.pth'))   # <<<<<< ADD THIS LINE\nmodel.load_state_dict(checkpoint['state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer'])\n\n# Test dataloader ==============================================================\ntest_dataset = Dataset(\n    list_IDs_test,\n    x_test_dir,\n    y_test_dir,\n    augmentation=get_validation_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    resize=(RESIZE),\n    to_categorical=False, # don't convert to onehot now\n    n_classes=n_classes,\n)\n\ntest_dataloader = DataLoader(test_dataset,\n                            batch_size=1,\n                            shuffle=False,\n                            num_workers=6)\n\n# Prediction ===================================================================\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix\nimport scipy.io as sio\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsave_pred = True\nthreshold = 0.5\nep = 1e-6\nraw_pred = []\n\nHARD_LINE = True\n\nsave_dir_pred = os.path.join('/kaggle/working', 'predictions', model_name)\nsave_dir_pred_pal = os.path.join('/kaggle/working', 'predictions_palette', model_name)\nsave_dir_pred_pal_cat = os.path.join('/kaggle/working', 'predictions_palette_cat', model_name)\n\nif not os.path.exists(save_dir_pred): os.makedirs(save_dir_pred)\nif not os.path.exists(save_dir_pred_pal): os.makedirs(save_dir_pred_pal)\nif not os.path.exists(save_dir_pred_pal_cat): os.makedirs(save_dir_pred_pal_cat)\n\n# Create a dictionary to store metrics\nmetric = {} # Nested metric format: metric[image_name][label] = [precision, recall, dice, iou]\n\n# fig, ax = plt.subplots(5,2, figsize=(10,15))\niter_test_dataloader = iter(test_dataloader)\n\n\nclass_names = [\n    \"background\",    # 0\n    \"granulation\",   # 1\n    \"callus\",        # 2\n    \"fibrin\",        # 3\n    \"necrotic\",      # 4\n    \"eschar\",        # 5\n    \"neodermis\",     # 6\n    \"tendon\",        # 7\n    \"dressing\"       # 8\n]\n\n\npalette = [\n    [0, 0, 0],         # 0: background\n    [255, 0, 0],       # 1: granulation\n    [255, 255, 0],     # 2: callus\n    [0, 255, 0],       # 3: fibrin\n    [255, 165, 0],     # 4: necrotic\n    [128, 0, 128],     # 5: eschar\n    [0, 255, 255],     # 6: neodermis\n    [255, 192, 203],   # 7: tendon\n    [0, 0, 255],       # 8: dressing\n]\n\nstp, stn, sfp, sfn = 0, 0, 0, 0\n\nfor i in range(len(list_IDs_test)):\n\n    tp, tn, fp, fn = 0, 0, 0, 0\n\n    name = os.path.splitext(list_IDs_test[i])[0] # remove extension\n\n    metric[name] = {} # Creating nested dictionary\n\n    # Image-wise mean of metrics\n    i_mp, i_mr, i_mdice, i_miou = [], [], [], []\n\n    image, gt_mask = next(iter_test_dataloader) # get image and mask as Tensors\n\n    # Note: Image shape: torch.Size([1, 3, 512, 512]) and mask shape: torch.Size([1, 1, 512, 512])\n\n    model.eval()\n    with torch.no_grad():\n        pr_mask = model(image.to(DEVICE))\n\n\n    # Convert from onehot\n    # gt_mask = torch.argmax(gt_mask_, dim=1)\n    pr_mask = torch.argmax(pr_mask, dim=1)\n\n\n    # pr_mask = torch.argmax(pr_mask, dim=1)\n\n    # Move to CPU and convert to numpy\n    gt_mask = gt_mask.squeeze().cpu().numpy()\n    gt_mask = np.asarray(gt_mask, dtype=np.int64) # convert to integer\n    pred = pr_mask.squeeze().cpu().numpy()\n\n    # Save raw prediction\n    if RAW_PREDICTION: raw_pred.append(pred)\n\n    # Modify prediction based on threshold\n    # pred = (pred >= threshold) * 1\n\n    # Save prediction as png\n    if save_pred:\n        \"Uncomment for non-palette\"\n        cv2.imwrite(os.path.join(save_dir_pred, list_IDs_test[i]), np.squeeze(pred).astype(np.uint8))\n\n        \"Uncomment for palette\"\n        # Palette original\n        pal_gt_mask = np.squeeze(gt_mask).astype(np.uint8)\n        pal_gt_mask = Image.fromarray(pal_gt_mask)\n        pal_gt_mask = pal_gt_mask.convert(\"P\")\n        pal_gt_mask.putpalette(np.array(palette, dtype=np.uint8))\n\n        # Palette prediction\n        pal_pred = np.squeeze(pred).astype(np.uint8)\n        pal_pred = Image.fromarray(pal_pred)\n        pal_pred = pal_pred.convert(\"P\")\n        pal_pred.putpalette(np.array(palette, dtype=np.uint8))\n\n        pal_pred.save(os.path.join(save_dir_pred_pal, list_IDs_test[i])) # store\n\n        # Concatenate gt and pred side by side\n        concat_pals = Image.new(\"RGB\", (pal_gt_mask.width+pal_gt_mask.width, pal_gt_mask.height), \"white\")\n        concat_pals.paste(pal_gt_mask, (0, 0))\n        concat_pals.paste(pal_pred, (pal_gt_mask.width, 0))\n\n        concat_pals.save(os.path.join(save_dir_pred_pal_cat, list_IDs_test[i])) # store\n\n    # Find labels in gt and prediction\n    lbl_gt = set(np.unique(gt_mask))\n    lbl_gt.discard(0) # remove 0. It is background\n    lbl_pred = set(np.unique(pred))\n    lbl_pred.discard(0) # remove 0. It is background\n\n    # All labels\n    all_lbls = lbl_gt.union(lbl_pred)\n\n    # Find labels that are not common in both gt and prediction. For such cases. IoU = 0\n    diff1 = lbl_gt - lbl_pred\n    diff2 = lbl_pred - lbl_gt\n    diffs = diff1.union(diff2) # labels that do not exist in either gt or prediction\n\n    # Labels that are in the gt but not in prediction are fn\n    if len(diff1) > 0:\n        for d1 in diff1:\n            fn_ = len(np.argwhere(gt_mask == d1))\n            fn += fn_\n            sfn += fn\n\n    # Labels that are in the prediction but not in gt are fp\n    if len(diff2) > 0:\n        for d2 in diff2:\n            fp_ = len(np.argwhere(pred == d2))\n            fp += fp_\n            sfp += fp\n\n    # Set IoU == 0 for such labels\n    if not len(diffs) == 0:\n        for diff in diffs:\n            p, r, dice, iou = 0, 0, 0, 0\n            metric[name][str(diff)] = [p, r, dice, iou]\n            print(\"%d %s: label: %s; Precision: %3.2f; Recall: %3.2f; Dice: %3.2f; IoU: %3.2f\"%(i+1, name, diff, p, r, dice, iou))\n\n    # Find labels that are common in both gt and prediction.\n    cmns = lbl_gt.intersection(lbl_pred)\n\n    # Iterate over common labels\n    for cmn in cmns:\n        gt_idx = np.where(gt_mask == cmn)\n        pred_idx = np.where(pred == cmn)\n\n        # Convert to [(x1,y1), (x2,y2), ...]\n        gt_lidx, pred_lidx = [], [] # List index\n\n        for i in range(len(gt_idx[0])):\n            gt_lidx.append((gt_idx[0][i], gt_idx[1][i]))\n\n        for i in range(len(pred_idx[0])):\n            pred_lidx.append((pred_idx[0][i], pred_idx[1][i]))\n\n        # Calculate metrics\n        gt_tidx = tuple(gt_lidx) # convert to tuple\n        pred_tidx = tuple(pred_lidx) # convert to tuple\n        tp_cord = set(gt_tidx).intersection(pred_tidx) # set operation\n        fp_cord = set(pred_tidx).difference(gt_tidx) # set operation\n        fn_cord = set(gt_tidx).difference(pred_tidx) # set operation\n\n        tp += len(tp_cord)\n        fp += len(fp_cord)\n        fn += len(fn_cord)\n\n        stp += tp\n        sfp += fp\n        sfn += fn\n\n        p = (tp/(tp + fp + ep)) * 100\n        r = (tp/(tp + fn + ep)) * 100\n        dice = (2 * tp / (2 * tp + fp + fn + ep)) * 100\n        iou = (tp/(tp + fp + fn + ep)) * 100\n\n        print(\"%d %s: label: %s; Precision: %3.2f; Recall: %3.2f; Dice: %3.2f; IoU: %3.2f\"%(i+1, name, cmn, p, r, dice, iou))\n\n        metric[name][str(cmn)] = [p, r, dice, iou]\n\n        # Keep appending metrics for all labels for the current image\n        i_mp.append(p)\n        i_mr.append(r)\n        i_mdice.append(dice)\n        i_miou.append(iou)\n\n\n# create json object from dictionary\nimport json\njson_write = json.dumps(metric)\nf = open(os.path.join(save_dir_pred, \"metric.json\"), \"w\")\nf.write(json_write)\nf.close()\n\n# Data-based evalutation\nsiou = (stp/(stp + sfp + sfn + ep))*100\nsprecision = (stp/(stp + sfp + ep))*100\nsrecall = (stp/(stp + sfn + ep))*100\nsdice = (2 * stp / (2 * stp + sfp + sfn))*100\n\nprint('siou:', siou)\nprint('sprecision:', sprecision)\nprint('srecall:', srecall)\nprint('sdice:', sdice)\n\n# Save data-based result in a text file\nwith open(os.path.join(save_dir_pred, 'result.txt'), 'w') as f:\n    print(f'iou = {siou}', file=f)\n    print(f'precision = {sprecision}', file=f)\n    print(f'recall = {srecall}', file=f)\n    print(f'dice = {sdice}', file=f)\n    print(f'best model epoch = {best_model_epoch}', file=f)\n    print(f'model name = {model_name}', file=f)","metadata":{"_uuid":"d5f2415b-95da-4b3c-b85b-a92761015a2d","_cell_guid":"414def9c-59bb-428a-acd4-01c929f067a2","trusted":true,"collapsed":false,"id":"mDygdp9qfXax","execution":{"iopub.status.busy":"2025-05-26T18:08:09.272193Z","iopub.status.idle":"2025-05-26T18:08:09.272411Z","shell.execute_reply.started":"2025-05-26T18:08:09.272303Z","shell.execute_reply":"2025-05-26T18:08:09.272313Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\n\n# Make sure 'model' is your trained model and 'checkpoint_loc' is defined (as in your notebook)\n\n# 1️⃣ Save as PyTorch .pth file (best for Streamlit with PyTorch)\npth_path = os.path.join(checkpoint_loc, 'best_model_streamlit.pth')\ntorch.save(model.state_dict(), pth_path)\nprint(f\"✅ Saved PyTorch model for Streamlit: {pth_path}\")\n\n# 2️⃣ Save as ONNX (optional, for use in other frameworks or AI APIs)\nonnx_path = os.path.join(checkpoint_loc, 'model_streamlit.onnx')\n# Use the appropriate input shape for your model\ndummy_input = torch.randn(1, 3, 256, 256).to(model.device if hasattr(model, 'device') else 'cpu')\nmodel = model.cpu()  # move model to CPU before exporting\ntorch.onnx.export(\n    model, dummy_input, onnx_path,\n    input_names=['input'], output_names=['output'],\n    opset_version=11\n)\nprint(f\"✅ Saved ONNX model: {onnx_path}\")","metadata":{"_uuid":"944e1baf-50fd-4fbe-b9b9-09b60b05816c","_cell_guid":"d7d2bac4-0b03-4dd8-9b94-0728ea626b0a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-26T18:08:09.273607Z","iopub.status.idle":"2025-05-26T18:08:09.273960Z","shell.execute_reply.started":"2025-05-26T18:08:09.273770Z","shell.execute_reply":"2025-05-26T18:08:09.273784Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Replace with your actual checkpoints directory\nchkpt_dir = \"/kaggle/working/checkpoints\"\n\nfor root, dirs, files in os.walk(chkpt_dir):\n    print(f\"Folder: {root}\")\n    for fname in files:\n        print(\"    \", fname)","metadata":{"_uuid":"29ce811c-dfde-4721-b96e-7b91945c3613","_cell_guid":"d9a6ce9f-5b2f-435e-adc4-8de692f22282","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-26T18:08:09.275643Z","iopub.status.idle":"2025-05-26T18:08:09.275989Z","shell.execute_reply.started":"2025-05-26T18:08:09.275801Z","shell.execute_reply":"2025-05-26T18:08:09.275816Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install Google Drive API libraries if not already\n!pip install --quiet google-api-python-client google-auth-httplib2 google-auth-oauthlib\n\nimport os\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\n\n# Set your model folder name (change if you use another checkpoint)\nmodel_name = 'MiT+pscse_padded_aug_mit_b3_sup_2025-05-26_15-04-55'\n\n# Path to your Kaggle-uploaded service account JSON key\nSERVICE_ACCOUNT_FILE = '/kaggle/input/json-key/our-velocity-460004-b1-5b54e5408593.json'\n\n# Your Google Drive folder ID\nFOLDER_ID = '10BSufw575gLava3ao51TFBX0BT2wgJ4s'\n\n# Authenticate and create the service\ncredentials = service_account.Credentials.from_service_account_file(\n    SERVICE_ACCOUNT_FILE,\n    scopes=['https://www.googleapis.com/auth/drive']\n)\nservice = build('drive', 'v3', credentials=credentials)\n\n# List all files you want to upload (absolute file paths and display names)\nfiles_to_upload = [\n    (f'/kaggle/working/checkpoints/{model_name}/best_model.pth', 'best_model.pth'),\n    (f'/kaggle/working/checkpoints/{model_name}/best_model_streamlit.pth', 'best_model_streamlit.pth'),\n    (f'/kaggle/working/checkpoints/{model_name}/model_streamlit.onnx', 'model_streamlit.onnx')\n]\n\n# Upload loop\nfor local_path, drive_filename in files_to_upload:\n    if os.path.exists(local_path):\n        file_metadata = {'name': drive_filename, 'parents': [FOLDER_ID]}\n        media = MediaFileUpload(local_path, resumable=True)\n        uploaded_file = service.files().create(\n            body=file_metadata, media_body=media, fields='id'\n        ).execute()\n        print(f\"✅ Uploaded {drive_filename} to Google Drive (File ID: {uploaded_file.get('id')})\")\n    else:\n        print(f\"❌ File not found: {local_path}\")","metadata":{"_uuid":"e3525ca3-b715-4db8-841b-b6b5f666bdcf","_cell_guid":"035d7366-c915-411b-bcbf-991cc555f613","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-26T18:08:09.277273Z","iopub.status.idle":"2025-05-26T18:08:09.277587Z","shell.execute_reply.started":"2025-05-26T18:08:09.277432Z","shell.execute_reply":"2025-05-26T18:08:09.277448Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"08245533-c88d-48c8-a354-c1da5f2240f4","_cell_guid":"15a2d924-56ea-4fae-8daf-2dabb2fdf3c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"b6993ca8-f7fa-43ee-9852-8ce2eab013e5","_cell_guid":"94930eab-e832-4142-aa1d-6c1225287cb1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}